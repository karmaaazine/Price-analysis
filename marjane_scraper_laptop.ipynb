{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BASE_URL = \"https://www.marjanemall.ma\"\n",
    "START_URL = \"https://www.marjanemall.ma/informatique-gaming/ordinateur/ordinateur-portable\"\n",
    "MAX_PAGES = 100\n",
    "OUTPUT_DIR = \"Data\"\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"MarjaneMall_Laptops.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium Setup\n",
    "options = Options()\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")  # Ignore SSL certificate errors\n",
    "service = Service(\"C:/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_product_name(name):\n",
    "    \"\"\"\n",
    "    Extract structured information from the product name.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"Brand\": None,\n",
    "        \"Model\": None,\n",
    "        \"Generation\": None,\n",
    "        \"Processor\": None,\n",
    "        \"RAM\": None,\n",
    "        \"Storage\": None,\n",
    "    }\n",
    "    product_name = name.title()\n",
    "    # Extract brand\n",
    "    brand_match = re.search(r'\\b(HP|Dell|Lenovo|Asus|Acer|Apple|Msi|Samsung|Toshiba|Vivobook|Huawei|Jemco|Microsoft|)\\b', product_name, re.IGNORECASE)\n",
    "    data[\"Brand\"] = brand_match.group(1) if brand_match else \"Unknown\"\n",
    "\n",
    "    # Extract model\n",
    "    model_match = re.search(r'(EliteBook|ThinkPad|Inspiron|Pavilion|IdeaPad|MacBook|Predator|ZenBook|Aspire|OMEN|ROG|Satellite|MateBook)?\\s?\\d+\\s?[A-Za-z]*', product_name, re.IGNORECASE)\n",
    "    data[\"Model\"] = model_match.group(0).strip() if model_match else \"Unknown\"\n",
    "\n",
    "    # Extract generation\n",
    "    generation_match = re.search(r'(\\d{1,2})(?:th|e)?\\s?(?:GEN|GÃ©n)?\\s?(?:Core)?', product_name, re.IGNORECASE)\n",
    "    data[\"Generation\"] = f\"{generation_match.group(1)}th Gen\" if generation_match else \"Unknown\"\n",
    "\n",
    "\n",
    "    # Extract processor\n",
    "    processor_match = re.search(r'(I[3579]-?\\d+[A-Za-z0-9]*)', product_name, re.IGNORECASE)\n",
    "    data[\"Processor\"] = processor_match.group(1) if processor_match else \"Unknown\"\n",
    "\n",
    "    # Extract RAM\n",
    "    ram_match = re.search(r'(\\d+)\\s?(Go|GB)', product_name, re.IGNORECASE)\n",
    "    data[\"RAM\"] = f\"{ram_match.group(1)}GB\" if ram_match else \"Unknown\"\n",
    "\n",
    "\n",
    "\n",
    "    # Extract storage\n",
    "    storage_match = re.search(r'(\\d+\\s?(Go|GB)\\s?(SSD|HDD))', product_name, re.IGNORECASE)\n",
    "    data[\"Storage\"] = storage_match.group(1).replace('Go', 'GB').strip() if storage_match else \"Unknown\"\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    \"\"\"\n",
    "    Fetches the page source using Selenium.\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    # Wait for the product grid to be loaded (example: adjust this if the class name is different)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CLASS_NAME, \"product-item-link\"))\n",
    "    )\n",
    "    return driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(html):\n",
    "    \"\"\"\n",
    "    Parses product information from the HTML.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Debug: Print a preview of the page to confirm if products are present\n",
    "    print(soup.prettify()[:1000])  # Show the first 1000 characters for debugging\n",
    "    \n",
    "    results = soup.find_all('div', {'class': 'product-item-details'})\n",
    "    print(f\"Found {len(results)} product entries.\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No products found. Check the HTML structure or URL.\")\n",
    "        return []\n",
    "\n",
    "    all_products = []\n",
    "    collection_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    for item in results:\n",
    "        try:\n",
    "            # Product Name\n",
    "            product_name_tag = item.find('a', {'class': 'product-item-link'})\n",
    "            product_name = product_name_tag.text.strip() if product_name_tag else 'N/A'\n",
    "            print(f\"Product Name: {product_name}\")  # Debugging product name\n",
    "\n",
    "            # Product Link\n",
    "            link_tag = product_name_tag\n",
    "            link = link_tag['href'] if link_tag else 'N/A'\n",
    "            if link and not link.startswith('http'):\n",
    "                link = BASE_URL + link\n",
    "\n",
    "            # Extract Initial Price\n",
    "            old_price_tag = item.find('span', {'class': 'old-price'})\n",
    "            if old_price_tag:\n",
    "                price_wrapper_tag = old_price_tag.find('span', {'class': 'price-wrapper'})\n",
    "                if price_wrapper_tag and 'data-price-amount' in price_wrapper_tag.attrs:\n",
    "                    price_initial = float(price_wrapper_tag['data-price-amount'])\n",
    "                else:\n",
    "                    price_initial = 'N/A'\n",
    "            else:\n",
    "                price_initial = 'N/A'\n",
    "\n",
    "            print(f\"Initial Price: {price_initial}\")\n",
    "            \n",
    "            # Promo Price (Optional)\n",
    "            price_promo_tag = item.find('span', {'class': 'price-wrapper'})\n",
    "            if price_promo_tag and 'data-price-amount' in price_promo_tag.attrs:\n",
    "                price_promo = float(price_promo_tag['data-price-amount'])\n",
    "            else:\n",
    "                price_promo = 'N/A'\n",
    "            \n",
    "                \n",
    "            promotions = []\n",
    "            promo_tags = item.find_all(\"span\", class_=\"octopia-discount percent\")  # Chercher toutes les balises avec classe \"tag\"\n",
    "            for promo_tag in promo_tags:\n",
    "                if promo_tag.text.strip():\n",
    "                    promotions.append(promo_tag.text.strip())\n",
    "\n",
    "            promotion = \", \".join(promotions) if promotions else 'Aucune'\n",
    "\n",
    "            if promotion == 'Aucune' :\n",
    "                price_initial = price_promo\n",
    "                price_promo = 'N/A' \n",
    "\n",
    "            structured_data = parse_product_name(product_name)\n",
    "\n",
    "            # Product Object\n",
    "            product = {\n",
    "                'name':product_name,\n",
    "                **structured_data,\n",
    "                'marketplace': 'Marjane Mall',\n",
    "                'category': 'PC Portables',\n",
    "                'link': link,\n",
    "                'priceInitial': price_initial,\n",
    "                'pricePromo': price_promo,\n",
    "                'promotiontype' : promotion,\n",
    "                'collectionTime': collection_time\n",
    "            }\n",
    "            all_products.append(product)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing product: {e}\")\n",
    "            print(f\"Product HTML: {item.prettify()[:500]}\")  # Debug the specific item\n",
    "            continue\n",
    "\n",
    "    return all_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page(soup):\n",
    "    \"\"\"\n",
    "    Identifies the URL for the next page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        next_button = soup.find('li', {'class': 'item pages-item-next'})\n",
    "        if next_button:\n",
    "            link_tag = next_button.find('a')\n",
    "            if link_tag and 'href' in link_tag.attrs:\n",
    "                next_page = link_tag['href']\n",
    "                if not next_page.startswith('http'):\n",
    "                    next_page = BASE_URL + next_page\n",
    "                return next_page\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding next page: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(products):\n",
    "    \"\"\"\n",
    "    Save product data to a single CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(products).to_csv(OUTPUT_FILE, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(products).to_csv(OUTPUT_FILE, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_products(all_products):\n",
    "    cleaned_products = [\n",
    "        product for product in all_products\n",
    "        if all(value != \"Unknown\" for value in product.values())\n",
    "    ]\n",
    "\n",
    "    print(f\"Removed {len(all_products) - len(cleaned_products)} rows with 'Unknown' values.\")\n",
    "    return cleaned_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # log_run()\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    page_count = 0\n",
    "    all_products = []\n",
    "\n",
    "    try:\n",
    "        current_url = START_URL\n",
    "\n",
    "        while current_url and page_count < MAX_PAGES:\n",
    "            print(f\"Fetching page {page_count + 1}: {current_url}\")\n",
    "            html = get_data(current_url)\n",
    "            products = parse(html)\n",
    "            all_products.extend(products)\n",
    "\n",
    "            page_count += 1\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            current_url = get_next_page(soup)\n",
    "            if not current_url:\n",
    "                print(\"No more pages available.\")\n",
    "                break\n",
    "\n",
    "        if all_products:\n",
    "            cleaned_file = clean_all_products(all_products)\n",
    "            save_to_csv(cleaned_file)\n",
    "            print(f\"Products saved to {OUTPUT_FILE}\")\n",
    "        else:\n",
    "            print(\"No products found.\")\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
